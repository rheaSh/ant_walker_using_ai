{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Ant to Walk using TD3 in MuJoCo Environment\n",
    "\n",
    "This Jupyter notebook showcases the implementation of the Twin Delayed Deep Deterministic (TD3) algorithm to train an ant to walk in the MuJoCo environment. TD3 is a state-of-the-art reinforcement learning algorithm designed for continuous control tasks. In this project, we leverage the power of PyTorch and various other libraries to create and train a neural network that learns to control the ant's movements in the MuJoCo simulation.\n",
    "\n",
    "### Dependencies\n",
    "Before we dive into the implementation, let's ensure that the necessary dependencies are installed. We'll be using PyTorch for deep learning, gym for the MuJoCo environment, and tensorboardX for visualizing training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902f3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import gym\n",
    "# import roboschool\n",
    "import pybullet\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install the required libraries using the following commands:\n",
    "\n",
    "\n",
    "pip install torch tensorboardX gym roboschool pybullet\n",
    "\n",
    "### MuJoCo Environment\n",
    "MuJoCo (Multi-Joint dynamics with Contact) is a physics engine designed for research and development in robotics, biomechanics, graphics, and machine learning. In this project, we will be using the MuJoCo environment provided by the gym library to simulate the ant's movements.\n",
    "\n",
    "Let's proceed with the implementation and training of the TD3 algorithm to observe how our neural network learns to control the ant's locomotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73ef773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Network for TD3 Algorithm\n",
    "The Actor class is an essential component of the Twin Delayed Deep Deterministic (TD3) algorithm. It serves as the policy network responsible for generating actions in the continuous action space. \n",
    "\n",
    "### Model Architecture\n",
    "The actor network consists of three fully connected layers:\n",
    "\n",
    "Input Layer (self.l1): This layer takes the state as input with a dimensionality of state_dim and outputs a feature representation with 400 nodes.      \n",
    "First Hidden Layer (self.l2): The first hidden layer has 400 nodes and applies the Rectified Linear Unit (ReLU) activation function to introduce non-linearity.     \n",
    "Second Hidden Layer (self.l3): The second hidden layer reduces the dimensionality to action_dim and applies the hyperbolic tangent (tanh) activation function. The final output is scaled by max_action to ensure it falls within the specified action range.\n",
    "\n",
    "Initialization Parameters\n",
    "\n",
    "The Actor class takes the following parameters during initialization:\n",
    "\n",
    "    state_dim: Dimension of each state.\n",
    "    action_dim: Dimension of each action.\n",
    "    max_action: Highest action value allowed.\n",
    "\n",
    "To obtain the actor's output (action), pass a state tensor through the network using the forward method. The output is a tensor with tanh activation, scaled by max_action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0497c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            action output of network with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic Network for TD3 Algorithm\n",
    "The Critic class plays a crucial role in the Twin Delayed Deep Deterministic (TD3) algorithm, serving as the value function approximator. This network evaluates the quality of the actions taken by the actor in a given state. \n",
    "\n",
    "### Model Architecture\n",
    "The critic network consists of two separate Q-value estimators (or, Q Learning networks):\n",
    "\n",
    "Q1 Architecture:\n",
    "\n",
    "Input Layer (self.l1): Concatenates the state and action tensors and passes through a fully connected layer with 400 nodes.\n",
    "First Hidden Layer (self.l2): Applies the Rectified Linear Unit (ReLU) activation function to introduce non-linearity.\n",
    "Second Hidden Layer (self.l3): Reduces the dimensionality to 1, providing the Q1 value.\n",
    "\n",
    "Q2 Architecture:\n",
    "\n",
    "Input Layer (self.l4): Similar to Q1, concatenates the state and action tensors and passes through a fully connected layer with 400 nodes.\n",
    "First Hidden Layer (self.l5): Applies ReLU activation.\n",
    "Second Hidden Layer (self.l6): Reduces the dimensionality to 1, providing the Q2 value.\n",
    "\n",
    "The Critic class takes the following parameters during initialization:\n",
    "\n",
    "    state_dim: Dimension of each state.\n",
    "    action_dim: Dimension of each action.\n",
    "\n",
    "To obtain the Q-values from both Q1 and Q2, pass the state and action tensors through the network using the forward method. Additionally, the Q1 method allows for retrieving Q1 values separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546f8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            max_action (float): highest action to take\n",
    "            seed (int): Random seed\n",
    "            h1_units (int): Number of nodes in first hidden layer\n",
    "            h2_units (int): Number of nodes in second hidden layer\n",
    "            \n",
    "        Return:\n",
    "            value output of network \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer for TD3 Algorithm\n",
    "The ReplayBuffer class is crucial for implementing the experience replay mechanism in the Twin Delayed Deep Deterministic (TD3) algorithm. This buffer stores tuples of experiences, allowing the agent to learn from past interactions. It also facilitates efficient training by breaking the temporal correlation between consecutive experiences, leading to more stable and effective learning in the TD3 algorithm.\n",
    "\n",
    "\n",
    "### Implementation Details\n",
    "The replay buffer is implemented based on the code from OpenAI's Baselines library. It stores tuples of the form (state, next_state, action, reward, done). The key functionalities of the ReplayBuffer include:\n",
    "\n",
    "Initialization: The buffer is initialized with a maximum size (max_size) to limit the number of experiences stored.\n",
    "\n",
    "Adding Data: The add method is used to add experience tuples to the buffer. If the buffer is full, it replaces old entries in a cyclic manner.\n",
    "\n",
    "Sampling Data: The sample method randomly selects a batch of experiences from the buffer for training the TD3 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8faa35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: \n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "# Expects tuples of (state, next_state, action, reward, done)\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=1000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_size (int): total amount of tuples to store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        \"\"\"Add experience tuples to buffer\n",
    "        \n",
    "        Args:\n",
    "            data (tuple): experience replay tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): size of sample\n",
    "        \"\"\"\n",
    "        \n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind: \n",
    "            s, a, s_, r, d = self.storage[i]\n",
    "            states.append(np.array(s, copy=False))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twin Delayed Deep Deterministic (TD3) Agent Implementation\n",
    "The TD3 class represents the agent responsible for training and selecting actions in the Twin Delayed Deep Deterministic (TD3) algorithm. This agent consists of an actor network, a critic network, and associated methods for training and action selection. This class encapsulates the key components of the TD3 algorithm, facilitating training and action selection for solving continuous control tasks in the given environment.\n",
    "\n",
    "### Initialization\n",
    "The agent is initialized with the following parameters:\n",
    "\n",
    "state_dim: Dimension of the state space.\n",
    "action_dim: Dimension of the action space.\n",
    "max_action: Highest possible action value.\n",
    "env: The Gym environment used for training.\n",
    "\n",
    "The select_action method is used to select actions based on the current state. It also supports the addition of noise to actions if needed.\n",
    "\n",
    "The train method is responsible for training both the actor and critic networks using experiences sampled from the replay buffer.\n",
    "\n",
    "The save and load methods allow for saving and loading the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43f798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGENT\n",
    "\n",
    "class TD3(object):\n",
    "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
    "    \n",
    "        Args:\n",
    "            state_dim (int): state size\n",
    "            action_dim (int): action size\n",
    "            max_action (float): highest action to take\n",
    "            device (device): cuda or cpu to process tensors\n",
    "            env (env): gym environment to use\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action, env):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.env = env\n",
    "\n",
    "\n",
    "        \n",
    "    def select_action(self, state, noise=0.1):\n",
    "        \"\"\"Select an appropriate action from the agent policy\n",
    "        \n",
    "            Args:\n",
    "                state (array): current state of environment\n",
    "                noise (float): how much noise to add to acitons\n",
    "                \n",
    "            Returns:\n",
    "                action (float): action clipped within action range\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if noise != 0: \n",
    "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
    "            \n",
    "        return action.clip(self.env.action_space.low, self.env.action_space.high)\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \"\"\"Train and update actor and critic networks\n",
    "        \n",
    "            Args:\n",
    "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
    "                iterations (int): how many times to run training\n",
    "                batch_size(int): batch size to sample from replay buffer\n",
    "                discount (float): discount factor\n",
    "                tau (float): soft update for main networks to target networks\n",
    "                \n",
    "            Return:\n",
    "                actor_loss (float): loss from actor network\n",
    "                critic_loss (float): loss from critic network\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer \n",
    "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(1 - d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Select action according to policy and add clipped noise \n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (done * discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(\"new\", \"actor.pth\")\n",
    "        torch.save(\"new\", \"critic.pth\")\n",
    "#         torch.save(self.actor.state_dict(), '%s\\%s_actor.pth' % (directory, filename))\n",
    "#         torch.save(self.critic.state_dict(), '%s\\%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
    "        torch.save(\"new\", \"actor.pth\")\n",
    "        torch.save(\"new\", \"critic.pth\")\n",
    "#         self.actor.load_state_dict(torch.load('%s\\%s_actor.pth' % (directory, filename)))\n",
    "#         self.critic.load_state_dict(torch.load('%s\\%s_critic.pth' % (directory, filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Runner Implementation\n",
    "The Runner class is responsible for executing environment steps and adding experiences to the replay buffer. It interacts with the environment using the agent's selected actions, collects observations, rewards, and updates the replay buffer. This class facilitates the interaction between the environment, the TD3 agent, and the replay buffer, ensuring that experiences are collected and stored for training the agent.\n",
    "\n",
    "Initialization:         \n",
    "The Runner is initialized with the following parameters:\n",
    "\n",
    "env: The Gym environment    \n",
    "agent: The TD3 agent    \n",
    "replay_buffer: The replay buffer for storing experiences    \n",
    "\n",
    "The next_step method is used to perform the next environment step. It takes the current number of timesteps in the episode (episode_timesteps) and an optional noise parameter. The method returns the reward obtained from the step and a boolean indicating whether the episode is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55372c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNER\n",
    "class Runner():\n",
    "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, replay_buffer):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.obs = env.reset()\n",
    "        self.done = False\n",
    "        \n",
    "    def next_step(self, episode_timesteps, noise=0.1):\n",
    "        \n",
    "        action = self.agent.select_action(np.array(self.obs), noise=0.1)\n",
    "        \n",
    "        # Perform action\n",
    "        new_obs, reward, done, _ = self.env.step(action) \n",
    "        done_bool = 0 if episode_timesteps + 1 == 200 else float(done)\n",
    "    \n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
    "        \n",
    "        self.obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            return reward, True\n",
    "        \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation Function\n",
    "The evaluate_policy function is designed to assess the performance of a given agent's policy over a specified number of evaluation episodes. It runs the agent's policy in the environment and computes the average reward. It runs the agent's policy for the specified number of evaluation episodes and returns the average reward. It also has an option to render the environment during evaluation for visual inspection.\n",
    "\n",
    "This function is valuable for assessing the performance of the trained agent in a quantitative manner, providing insights into its effectiveness in the given environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df845cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "\n",
    "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
    "    \"\"\"run several episodes using the best agent policy\n",
    "        \n",
    "        Args:\n",
    "            policy (agent): agent to evaluate\n",
    "            env (env): gym environment\n",
    "            eval_episodes (int): how many test episodes to run\n",
    "            render (bool): show training\n",
    "        \n",
    "        Returns:\n",
    "            avg_reward (float): average reward over the number of evaluations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = policy.select_action(np.array(obs), noise=0)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Function\n",
    "The observe function is designed to populate the replay buffer by running episodes while taking random actions. This process helps the agent gather diverse experiences for training.\n",
    "\n",
    "This function runs episodes, taking random actions in the environment, and adds the corresponding experiences to the replay buffer. This pre-filling of the buffer with diverse experiences helps bootstrap the training of the agent.\n",
    "\n",
    "It's important to observe enough steps to provide a rich set of experiences for the agent to learn from during the initial stages of training. The progress is printed in the console for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e48b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION\n",
    "def observe(env,replay_buffer, observation_steps):\n",
    "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
    "    \n",
    "        Args:\n",
    "            env (env): gym environment\n",
    "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
    "            observation_steps (int): how many steps to observe for\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    time_steps = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while time_steps < observation_steps:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
    "\n",
    "        obs = new_obs\n",
    "        time_steps += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "The train function is responsible for training the agent using the Twin Delayed Deep Deterministic (TD3) algorithm for a specified number of exploration steps. It performs training iterations, evaluates the agent periodically, and logs relevant metrics.\n",
    "\n",
    "This function iterates through exploration steps, updating the agent's policy using experiences from the replay buffer. It also evaluates the agent periodically and saves the best model based on the evaluation reward. Training progress is logged using TensorBoard.\n",
    "\n",
    "To experiment with different instances of the TD3, adjust the hyperparameters and replace placeholder values according to your specific use case and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "def train(agent, test_env):\n",
    "    \"\"\"Train the agent for exploration steps\n",
    "    \n",
    "        Args:\n",
    "            agent (Agent): agent to use\n",
    "            env (environment): gym environment\n",
    "            writer (SummaryWriter): tensorboard writer\n",
    "            exploration (int): how many training steps to run\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    done = False \n",
    "    obs = env.reset()\n",
    "    evaluations = []\n",
    "    rewards = []\n",
    "    best_avg = -2000\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-TD3_Baseline_HalfCheetah\")\n",
    "    \n",
    "    while total_timesteps < EXPLORATION:\n",
    "    \n",
    "        if done: \n",
    "\n",
    "            if total_timesteps != 0: \n",
    "                rewards.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards[-100:])\n",
    "                \n",
    "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
    "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
    "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
    "                \n",
    "                if best_avg < avg_reward:\n",
    "                    best_avg = avg_reward\n",
    "                    print(\"saving best model....\\n\")\n",
    "                    agent.save(\"best_avg\",\"new\")\n",
    "\n",
    "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
    "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "                if avg_reward >= REWARD_THRESH:\n",
    "                    break\n",
    "\n",
    "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
    "\n",
    "                # Evaluate episode\n",
    "                if timesteps_since_eval >= EVAL_FREQUENCY:\n",
    "                    timesteps_since_eval %= EVAL_FREQUENCY\n",
    "                    eval_reward = evaluate_policy(agent, test_env)\n",
    "                    evaluations.append(avg_reward)\n",
    "                    writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
    "\n",
    "                    if best_avg < eval_reward:\n",
    "                        best_avg = eval_reward\n",
    "                        print(\"saving best model....\\n\")\n",
    "                        agent.save(\"best_avg\",\"new\")\n",
    "\n",
    "                episode_reward = 0\n",
    "                episode_timesteps = 0\n",
    "                episode_num += 1 \n",
    "\n",
    "        reward, done = runner.next_step(episode_timesteps)\n",
    "        episode_reward += reward\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Training Setup\n",
    "The following code initializes the environment, sets seeds, and sets up the necessary components for training.\n",
    "\n",
    "The provided code initializes the environment, sets up the necessary components, and prepares for training using the TD3 algorithm. The training loop can be extended based on your specific requirements and evaluation criteria. Adjust the hyperparameters and replace placeholder values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21e00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "\n",
    "ENV = \"Ant-v2\"\n",
    "SEED = 0\n",
    "OBSERVATION = 10000\n",
    "EXPLORATION = 70000\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.05\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "POLICY_FREQUENCY = 2\n",
    "EVAL_FREQUENCY = 5000\n",
    "REWARD_THRESH = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n"
     ]
    }
   ],
   "source": [
    "# Create the Gym environment and set the device (GPU or CPU)\n",
    "env = gym.make(ENV)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "env.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Obtain state and action dimensions along with the maximum action value from the environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# Create the TD3 policy using the obtained dimensions and maximum action value\n",
    "policy = TD3(state_dim, action_dim, max_action, env)\n",
    "\n",
    "# Create a replay buffer to store experiences for training\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# Create a Runner instance to handle environment steps and experience collection\n",
    "runner = Runner(env, policy, replay_buffer)\n",
    "\n",
    "# Initialize variables for tracking training progress\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c5298c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Buffer 10000/10000."
     ]
    }
   ],
   "source": [
    "# Populate replay buffer\n",
    "observe(env, replay_buffer, OBSERVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48c8bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best model....\n",
      "\n",
      "Total T: 5000 Episode Num: 4 Reward: -2593.868000 Avg Reward: -1756.026355\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -2381.210180\n",
      "---------------------------------------\n",
      "Total T: 10203 Episode Num: 12 Reward: -2700.322466 Avg Reward: -1743.057244\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -1407.382413\n",
      "---------------------------------------\n",
      "Total T: 15514 Episode Num: 22 Reward: -2631.763569 Avg Reward: -1595.317124\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -3003.934923\n",
      "---------------------------------------\n",
      "Total T: 20116 Episode Num: 32 Reward: -2677.090817 Avg Reward: -1483.324740\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -673.511732\n",
      "---------------------------------------\n",
      "Total T: 25014 Episode Num: 110 Reward: -43.153825 Avg Reward: -413.86411692\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -113.430779\n",
      "---------------------------------------\n",
      "Total T: 30433 Episode Num: 319 Reward: -2531.474277 Avg Reward: -90.501846\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -417.158667\n",
      "---------------------------------------\n",
      "Total T: 35003 Episode Num: 421 Reward: -60.774830 Avg Reward: -98.980055061\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -95.957487\n",
      "---------------------------------------\n",
      "Total T: 40821 Episode Num: 447 Reward: 310.459374 Avg Reward: -68.49785801\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 258.856561\n",
      "---------------------------------------\n",
      "Total T: 45822 Episode Num: 453 Reward: 484.856195 Avg Reward: -51.390362\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 570.508782\n",
      "---------------------------------------\n",
      "Total T: 50823 Episode Num: 459 Reward: 550.846220 Avg Reward: -18.192246\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 582.192776\n",
      "---------------------------------------\n",
      "Total T: 55824 Episode Num: 465 Reward: -1533.310231 Avg Reward: -8.045332\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: -162.599762\n",
      "---------------------------------------\n",
      "Total T: 60100 Episode Num: 472 Reward: 122.065936 Avg Reward: -13.1199236\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 8.258576\n",
      "---------------------------------------\n",
      "Total T: 65107 Episode Num: 481 Reward: 524.000055 Avg Reward: 17.3499012\n",
      "---------------------------------------\n",
      "Evaluation over 100 episodes: 592.063344\n",
      "---------------------------------------\n",
      "Total T: 69108 Episode Num: 486 Reward: 468.254895 Avg Reward: 40.199123"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "train(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to save the policy, and load it. Further steps, such as those to evaluate policy for n steps are useful once the policy runs satisfactorily.\n",
    "The demo of the walking ant was taken by running this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "627308bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all networks\n",
    "torch.save(policy.actor.state_dict(), \"actor.pth\")\n",
    "torch.save(policy.critic.state_dict(), \"critic.pth\")\n",
    "\n",
    "# Load trained policy\n",
    "policy.load()\n",
    "policy.actor.load_state_dict(torch.load('actor.pth'))\n",
    "policy.critic.load_state_dict(torch.load('critic.pth'))\n",
    "\n",
    "# Watch the trained agent run \n",
    "for i in range(10):\n",
    "    evaluate_policy(policy, env, render=True)\n",
    "\n",
    "# Shut down the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b1d3d",
   "metadata": {},
   "source": [
    "The best possible policy was obtained by a TD3 instance using the below configurations. The console output for its run was as below.\n",
    "\n",
    "\n",
    "```\n",
    "WITH TAU = 0.005\n",
    "\n",
    "Total T: 5000 Episode Num: 4 Reward: 621.704693 Avg Reward: 715.725392\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 472.979710\n",
    "---------------------------------------\n",
    "Total T: 10897 Episode Num: 18 Reward: 314.681440 Avg Reward: 261.822236\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 275.993126\n",
    "---------------------------------------\n",
    "Total T: 15234 Episode Num: 27 Reward: 200.160667 Avg Reward: 239.144880\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 208.402902\n",
    "---------------------------------------\n",
    "Total T: 20046 Episode Num: 40 Reward: 499.843529 Avg Reward: 222.455397\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 350.905543\n",
    "---------------------------------------\n",
    "Total T: 25004 Episode Num: 51 Reward: 46.759798 Avg Reward: 224.2817825\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 289.261309\n",
    "---------------------------------------\n",
    "Total T: 30097 Episode Num: 58 Reward: 278.929342 Avg Reward: 246.330099\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 419.907933\n",
    "---------------------------------------\n",
    "Total T: 35990 Episode Num: 69 Reward: 491.753643 Avg Reward: 255.749711\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 374.798351\n",
    "---------------------------------------\n",
    "Total T: 40250 Episode Num: 78 Reward: 481.119700 Avg Reward: 261.986176\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 402.360503\n",
    "---------------------------------------\n",
    "Total T: 45783 Episode Num: 87 Reward: 627.406303 Avg Reward: 275.038945\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 405.799572\n",
    "---------------------------------------\n",
    "Total T: 50195 Episode Num: 96 Reward: 105.710424 Avg Reward: 279.954127\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 398.072862\n",
    "---------------------------------------\n",
    "Total T: 55064 Episode Num: 111 Reward: 806.813517 Avg Reward: 266.249092\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 429.293722\n",
    "---------------------------------------\n",
    "Total T: 60430 Episode Num: 123 Reward: 796.467087 Avg Reward: 288.427197\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 734.379806\n",
    "---------------------------------------\n",
    "Total T: 65308 Episode Num: 136 Reward: 615.881982 Avg Reward: 310.279847\n",
    "---------------------------------------\n",
    "Evaluation over 100 episodes: 696.263060\n",
    "---------------------------------------\n",
    "Total T: 69264 Episode Num: 141 Reward: 838.117703 Avg Reward: 334.0237282\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
